{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the python notebook to train various ML models for ai face image identification task. The first step is to download the dataset using the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /Users/guobuzai/miniconda3/envs/ml/lib/python3.12/site-packages (1.6.17)\n",
      "Requirement already satisfied: six>=1.10 in /Users/guobuzai/miniconda3/envs/ml/lib/python3.12/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /Users/guobuzai/miniconda3/envs/ml/lib/python3.12/site-packages (from kaggle) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil in /Users/guobuzai/miniconda3/envs/ml/lib/python3.12/site-packages (from kaggle) (2.9.0)\n",
      "Requirement already satisfied: requests in /Users/guobuzai/miniconda3/envs/ml/lib/python3.12/site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/guobuzai/miniconda3/envs/ml/lib/python3.12/site-packages (from kaggle) (4.66.5)\n",
      "Requirement already satisfied: python-slugify in /Users/guobuzai/miniconda3/envs/ml/lib/python3.12/site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in /Users/guobuzai/miniconda3/envs/ml/lib/python3.12/site-packages (from kaggle) (2.2.2)\n",
      "Requirement already satisfied: bleach in /Users/guobuzai/miniconda3/envs/ml/lib/python3.12/site-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: webencodings in /Users/guobuzai/miniconda3/envs/ml/lib/python3.12/site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /Users/guobuzai/miniconda3/envs/ml/lib/python3.12/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/guobuzai/miniconda3/envs/ml/lib/python3.12/site-packages (from requests->kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/guobuzai/miniconda3/envs/ml/lib/python3.12/site-packages (from requests->kaggle) (3.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, tarfile, kaggle, zipfile; \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"ai_faces\": \"chelove4draste/5k-ai-generated-faces\",\n",
    "    \"real_faces\": \"atulanandjha/lfwpeople\"\n",
    "}\n",
    "\n",
    "# Local download paths\n",
    "download_dir = \"./datasets\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download and unzip Kaggle datasets\n",
    "def download_and_extract(dataset_name, kaggle_path, target_dir):\n",
    "    print(f\"Downloading {dataset_name}...\")\n",
    "    kaggle.api.dataset_download_files(kaggle_path, path=target_dir, unzip=True)\n",
    "    print(f\"{dataset_name} downloaded and extracted to {target_dir}\")\n",
    "# Function to check if a dataset is already downloaded\n",
    "def is_dataset_downloaded(target_dir):\n",
    "    # Check if the directory exists and contains files\n",
    "    return os.path.exists(target_dir) and any(os.scandir(target_dir))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ai_faces...\n",
      "Dataset URL: https://www.kaggle.com/datasets/chelove4draste/5k-ai-generated-faces\n",
      "ai_faces downloaded and extracted to ./datasets/ai_faces\n",
      "Downloading real_faces...\n",
      "Dataset URL: https://www.kaggle.com/datasets/atulanandjha/lfwpeople\n",
      "real_faces downloaded and extracted to ./datasets/real_faces\n",
      "Datasets are checked, downloaded, and organized.\n"
     ]
    }
   ],
   "source": [
    "# Download and extract datasets if not already downloaded\n",
    "for name, kaggle_path in datasets.items():\n",
    "    dataset_path = os.path.join(download_dir, name)\n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "    \n",
    "    if is_dataset_downloaded(dataset_path):\n",
    "        print(f\"{name} dataset is already downloaded at {dataset_path}.\")\n",
    "    else:\n",
    "        download_and_extract(name, kaggle_path, dataset_path)\n",
    "\n",
    "print(\"Datasets are checked, downloaded, and organized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted ./datasets/real_faces/lfw-funneled.tgz to ./datasets/real_faces\n"
     ]
    }
   ],
   "source": [
    "lfw_tgz_path = os.path.join(download_dir, \"real_faces\", \"lfw-funneled.tgz\")\n",
    "extract_path = os.path.join(download_dir, \"real_faces\")\n",
    "\n",
    "# Function to extract .tgz files\n",
    "def extract_tgz(file_path, target_dir):\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "    with tarfile.open(file_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=target_dir)\n",
    "    print(f\"Extracted {file_path} to {target_dir}\")\n",
    "\n",
    "# Extract the LFW dataset\n",
    "extract_tgz(lfw_tgz_path, extract_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all the initial datasets are downloaded. We will load 4000 of each of the dataset and then randomly pick 2500 of each to be the trainingset, and 750 of each to be validation set, and the rest to be test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing files in directory: ./datasets/ai_faces/5k\n",
      "Found 5000 files in AI faces directory.\n",
      "First 5 files: ['./datasets/ai_faces/5k/seed303843.png', './datasets/ai_faces/5k/seed302585.png', './datasets/ai_faces/5k/seed300392.png', './datasets/ai_faces/5k/seed301932.png', './datasets/ai_faces/5k/seed917028.png']\n",
      "Listing files in directory: ./datasets/real_faces/lfw_funneled\n",
      "Found 13233 files in Real faces directory.\n",
      "First 5 files: ['./datasets/real_faces/lfw_funneled/German_Khan/German_Khan_0001.jpg', './datasets/real_faces/lfw_funneled/Stefano_Gabbana/Stefano_Gabbana_0001.jpg', './datasets/real_faces/lfw_funneled/Dragan_Covic/Dragan_Covic_0001.jpg', './datasets/real_faces/lfw_funneled/Jeff_Hornacek/Jeff_Hornacek_0001.jpg', './datasets/real_faces/lfw_funneled/Sureyya_Ayhan/Sureyya_Ayhan_0001.jpg']\n"
     ]
    }
   ],
   "source": [
    "ai_faces_dir = \"./datasets/ai_faces/5k\"\n",
    "real_faces_dir = \"./datasets/real_faces/lfw_funneled\"\n",
    "\n",
    "# Function to list files in a directory\n",
    "def list_files(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory does not exist: {directory}\")\n",
    "        return []\n",
    "    print(f\"Listing files in directory: {directory}\")\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(directory):  # Walk through the directory\n",
    "        for file in filenames:\n",
    "            if file.endswith(('.png', '.jpg', '.jpeg')):  # Filter for image files\n",
    "                files.append(os.path.join(root, file))\n",
    "    return files\n",
    "\n",
    "# List files in AI faces directory\n",
    "ai_faces_files = list_files(ai_faces_dir)\n",
    "print(f\"Found {len(ai_faces_files)} files in AI faces directory.\")\n",
    "print(f\"First 5 files: {ai_faces_files[:5]}\")\n",
    "\n",
    "# List files in Real faces directory\n",
    "real_faces_files = list_files(real_faces_dir)\n",
    "print(f\"Found {len(real_faces_files)} files in Real faces directory.\")\n",
    "print(f\"First 5 files: {real_faces_files[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/guobuzai/miniconda3/envs/ml/lib/python3.12/site-packages (2.1.1)\n",
      "Requirement already satisfied: pillow in /Users/guobuzai/miniconda3/envs/ml/lib/python3.12/site-packages (10.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy \n",
    "!pip install pillow\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess images into an array\n",
    "def load_images_to_array(file_paths, img_size):\n",
    "    images = []\n",
    "    for filepath in file_paths:\n",
    "        try:\n",
    "            img = Image.open(filepath).convert(\"RGB\")  # Ensure RGB format\n",
    "            img = img.resize(img_size)  # Resize to target size\n",
    "            images.append(np.array(img) / 255.0)  # Normalize pixel values\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filepath}: {e}\")\n",
    "    return np.array(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI faces array shape: (2000, 128, 128, 3)\n",
      "Real faces array shape: (2000, 128, 128, 3)\n",
      "Images have been processed and saved to numpy arrays.\n"
     ]
    }
   ],
   "source": [
    "# Define constants\n",
    "IMG_SIZE = (128, 128)  # Resize all images to 128x128\n",
    "MAX_IMAGES = 2000  # Limit the number of images\n",
    "\n",
    "# Preprocess AI faces\n",
    "ai_faces_array = load_images_to_array(ai_faces_files[:MAX_IMAGES], IMG_SIZE)\n",
    "print(f\"AI faces array shape: {ai_faces_array.shape}\")\n",
    "\n",
    "# Preprocess Real faces\n",
    "real_faces_array = load_images_to_array(real_faces_files[:MAX_IMAGES], IMG_SIZE)\n",
    "print(f\"Real faces array shape: {real_faces_array.shape}\")\n",
    "\n",
    "# Save to numpy arrays\n",
    "np.save(\"ai_faces.npy\", ai_faces_array)\n",
    "np.save(\"real_faces.npy\", real_faces_array)\n",
    "\n",
    "print(\"Images have been processed and saved to numpy arrays.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Train: (1250, 128, 128, 3), Validation: (400, 128, 128, 3), Test: (350, 128, 128, 3)\n",
      "Real Train: (1250, 128, 128, 3), Validation: (400, 128, 128, 3), Test: (350, 128, 128, 3)\n",
      "Training Set: X_train: (2500, 128, 128, 3), y_train: (2500,)\n",
      "Validation Set: X_val: (800, 128, 128, 3), y_val: (800,)\n",
      "Test Set: X_test: (700, 128, 128, 3), y_test: (700,)\n"
     ]
    }
   ],
   "source": [
    "# Load the numpy arrays\n",
    "ai_faces = np.load(\"ai_faces.npy\")\n",
    "real_faces = np.load(\"real_faces.npy\")\n",
    "\n",
    "# Define dataset sizes\n",
    "train_size = 1250\n",
    "val_size = 400\n",
    "\n",
    "# Function to split the data randomly\n",
    "def split_data(data, train_size, val_size):\n",
    "    indices = np.random.permutation(len(data))  # Shuffle indices\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:train_size + val_size]\n",
    "    test_indices = indices[train_size + val_size:]\n",
    "    return data[train_indices], data[val_indices], data[test_indices]\n",
    "\n",
    "# Split AI faces\n",
    "ai_train, ai_val, ai_test = split_data(ai_faces, train_size, val_size)\n",
    "print(f\"AI Train: {ai_train.shape}, Validation: {ai_val.shape}, Test: {ai_test.shape}\")\n",
    "\n",
    "# Split Real faces\n",
    "real_train, real_val, real_test = split_data(real_faces, train_size, val_size)\n",
    "print(f\"Real Train: {real_train.shape}, Validation: {real_val.shape}, Test: {real_test.shape}\")\n",
    "\n",
    "# Combine training, validation, and test sets\n",
    "X_train = np.concatenate((ai_train, real_train), axis=0)\n",
    "y_train = np.array([0] * len(ai_train) + [1] * len(real_train))  # 0 = AI, 1 = Real\n",
    "\n",
    "X_val = np.concatenate((ai_val, real_val), axis=0)\n",
    "y_val = np.array([0] * len(ai_val) + [1] * len(real_val))\n",
    "\n",
    "X_test = np.concatenate((ai_test, real_test), axis=0)\n",
    "y_test = np.array([0] * len(ai_test) + [1] * len(real_test))\n",
    "\n",
    "# Shuffle the datasets for better training\n",
    "train_indices = np.random.permutation(len(X_train))\n",
    "val_indices = np.random.permutation(len(X_val))\n",
    "test_indices = np.random.permutation(len(X_test))\n",
    "\n",
    "X_train, y_train = X_train[train_indices], y_train[train_indices]\n",
    "X_val, y_val = X_val[val_indices], y_val[val_indices]\n",
    "X_test, y_test = X_test[test_indices], y_test[test_indices]\n",
    "\n",
    "# Output the shapes of the final datasets\n",
    "print(f\"Training Set: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Validation Set: X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"Test Set: X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
